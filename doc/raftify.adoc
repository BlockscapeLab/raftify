:hardbreaks:

= Raftify Documentation

_Raftify_ is a Go implementation of the Raft leader election algorithm enhanced with an additional prevote phase pointed out in Henrik Ingo’s https://openlife.cc/system/files/4-modifications-for-Raft-consensus.pdf[_Four modifications for the Raft consensus algorithm_] paper. It enables the creation of a self-managing cluster of nodes by transforming an application into a Raft node. Raftify is meant to be a more _cost-efficient_ alternative to running a validator cluster with a separate full-fledged Raft cluster and a permission log.

This Go package is designed to be directly embedded into an application and provide a direct way of communicating between individual nodes, omitting the additional overhead caused by replicating a log.

== Concepts

Raftify takes the leader election algorithm of the Raft consensus and extends its functionality with a couple of new concenptual layers for increased robustness.

=== Leader Stickiness

The first extension to the default Raft leader election algorithm is the prevote phase which solves the "leader flip-flopping" problem which describes a new leader being elected in quick succession due to the connection breaking down ever so often between two competing leader nodes. Raftify solves this problem by introducing a new node state, making it a total of four: _Leader_, _Follower_, _PreCandidate_ and _Candidate_.

* A *leader* is the sole managing member of the cluster, in charge of sending out heartbeat messages to the rest of the cluster in order to signal availability.
* A *follower* is the receiving end of the heartbeat messages a leader sends out. Upon receival, it resets its so-called _election timeout_ (800-1200ms with best performance option) which is the minimum time to wait for a heartbeat message to arrive.
* A follower turns into a *precandidate* if its election timeout elapses which essentially means that it has not received any sign of availability from the current leader for an extended time period. It then starts collecting prevotes in order to make sure the other nodes haven’t heard back from the leader either and the leader is in fact unavailable, not just from its own perspective but from the other node’s perspective, too.
* A precandidate turns into a *candidate* once it has collected the majority of prevotes from the cluster members which marks the start signal for a new candidacy. it is now the candidate’s task to collect votes from the majority of cluster members in order to be promoted to the new leader.

During an election, each node can only vote for one candidate. A candidate always votes for itself. In order to be elected leader, a candidate must reach a quorum of `(n/2)+1` votes. If no leader can be elected for an extended amount of time, a new candidacy term is started and the process repeats.

=== Network Partition Tolerance

The second extension Raftify implements is a set of mechanisms that allows the cluster to continue operation during network partitions while keeping the single-leader promise.

==== Leader Self-Awareness

Network partitions cause connections between nodes to break down and split the cluster into multiple smaller sub-clusters. By default, Raft allows nodes in a sub-cluster to elect a new leader if the quorum of votes needed for promotion can be reached, thus making it possible for more than one leader to be elected at the same time. This poses a huge problem to a validator cluster which must never have more than one leader in order not to double-sign.

Raftify solves this problem by means of a _leader quorum_. Every single time a leader sends out heartbeat to all of the other follower nodes, it keeps track of which node has replied in a short time frame (200ms with best performance option). If the leader can't get a quorum of heartbeat replies three times in a row, it will suspect it got split off into a sub-cluster which cannot reach the quorum anymore. This, in turn, means that the other sub-cluster may still be able to reach the quorum and elect a new leader, so it voluntarily steps down after 3x200ms which is before the minimum election timeout of 800ms on any other cluster node runs out, keeping the promise of one single leader at all times.

==== Two-Stage Quorum

Raftify uses hashicorp's memberlist library to manage cluster membership and failure detection which means that it detects unavailable nodes and kicks them after a short period of time, thus enabling dynamic resizing of the cluster for failure-related events. By extension, this dynamically changing cluster size also affects the cluster's quorum. This bears the risk that a long lasting network partition could permanently split a cluster into two independent clusters which would both be able to reach their own quorum and elect their own leader.

This is where the two-stage quorum comes into play. It simply keeps the quorum of the previous cluster size as a requirement for establishing the quorum for the new/reduced cluster size. This way, even split-off partitions with an adjusted cluster size won't be able to elect a new leader.

== Limitations

* A cluster size of n can tolerate up to `floor((n-1)/2)`` node failures.
** Example: A cluster of 5 nodes tolerates `floor((5-1)/2) = 2` node failures.
* There must never fail more than `floor((n-1)/2)`_` nodes at the same time. Once the failed nodes are kicked out of the memberlist and the size shrinks, the tolerance resets to the new reduced cluster size.
** Example 1: If in a cluster of 5 nodes 3 nodes fail in a short time frame, the remaining 2 nodes will never be able to reach the quorum again in order to negotiate a new leader.
** Example 2: If in a cluster of 5 nodes 2 nodes fail in a short time frame, the remaining 3 nodes will still be able to reach the quorum in order to negotiate a new leader. The crashed nodes will eventually be kicked from the memberlist, thus shrinking the cluster size to a total of 3 nodes and adjusting its failure tolerance to `floor((3-1)/2) = 1` node.

== Configuration Reference

The configuration is to be provided in a `raftify.json` file. It needs to be put into the working directory specified in the second parameter of the `InitNode` method. For Gaia, this would be `~/.gaiad/config/`.

[cols="1,1,5"]
|===
|Key|Value|Description

|id|string|*(Mandatory)* The node’s identifier.
Must be unique.

|max_nodes|int|*(Mandatory)* The self-imposed limit of nodes to be run in the cluster.
Must be greater than 0.

|expect|int|*(Mandatory)* The number of nodes expected to be online in order to bootstrap the cluster and start the leader election. Once the expected number of nodes is online, all cluster members will be started simultaneously.
Must be 1 or higher.
*WARNING:* Please use `expect = 1` for single-node setups only. If you plan on running more than one node, set the `expect` value to the final cluster size on **ALL** nodes. 

|encrypt|string|_(Optional)_ The hex representation of the secret key used to encrypt messages.
The value must be either 16, 24 or 32 bytes to select AES-128, AES-192 or AES-256. Use https://www.browserling.com/tools/random-bytes[this tool] to generate a key.

|performance|int|_(Optional)_ The modifier used to multiply the maximum and minimum timeout and ticker settings. Higher values increase leader stability and reduce bandwidth and CPU but also increase the time needed to recover from a leader failure.
Must be 1 or higher. Defaults to 1 which is also the maximum performance setting.

|log_level|string|_(Optional)_ The minimum log level for console log messages. Can be DEBUG, INFO, WARN, ERR. Defaults to WARN.

|bind_addr|string|_(Optional)_ The address to bind to.
Defaults to 0.0.0.0 (all interfaces).

|bind_port|string|_(Optional)_ The port to bind to.
Defaults to 7946 (default port of memberlist).

|peer_list|[]string|_(Optional)_ The list of IP addresses of all cluster members (optionally including the address of the local node). It is used to determine the quorum in a non-bootstrapped cluster.
For example, if your peerlist has `n = 3` nodes then `floor((n/2)+1) = 2` nodes will need to be up and running to bootstrap the cluster.
Addresses must be provided in the `host:port` format.
Must not be empty if more than one node is expected.

|===

=== API

[source,go]
----
func InitNode(logger *log.Logger, workingDir string) (*Node, error)
----

Initializes a new Raftify node. Blocks until the cluster is successfully bootstrapped.

[source,go]
----
func (n *Node) Shutdown() error
----

Gracefully shuts down the Raftify node. All timers/tickers and listeners are stopped, channels are closed and the node leaves the cluster.

[source,go]
----
func (n *Node) GetHealthScore() int
----

Returns the health score which is a metric from the hashicorp/memberlist library. Lower numbers
are better, and 0 means "totally healthy".

[source,go]
----
func (n *Node) GetMembers() map[string]string
----

Returns a map of all members listed in the local memberlist with their respective id and address.

[source,go]
----
func (n *Node) GetState() State
----

Returns the node's current state which is either Leader, Follower, PreCandidate or Candidate.

== Optional Features/Improvements

[cold="3*"]
|===
|Current state|Proposed changes|Desired effect

|Intended and unintended leave events are internally handled the same. There’s no difference between a node being shut down and a crashed node leaving the cluster.|Implement custom message to be broadcasted alongside the default events that triggers an immediate change of the cluster size for intended leave events and therefore also the quorum.|Makes sure that only failover scenarios are backed by the constraint of having to reach the quorum of the previous cluster size. A cluster with 2 nodes for example could be shrunk to a single-node cluster and keep running despite the majority of nodes taken offline.

|Once the expected number of nodes are online and the cluster is bootstrapped, the nodes go through the full election process in order to elect their first leader.|Make the first node to start up the first leader on successful bootstrap. This can be measured by how many peers could be reached. If a node reaches no peers, it means that it started up first and thus it will skip the precandidate and candidate states and immediately become the first leader.|This skips the delay associated with the prevoting and voting phase needed to elect the first leader in order to get things going. This saves a few seconds on startup at best, so it’s nice to have.
|===
