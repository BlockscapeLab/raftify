:hardbreaks:

= Raftify Documentation

_Raftify_ is a Go implementation of the https://raft.github.io/[Raft] leader election algorithm enhanced with an additional prevote phase pointed out in Henrik Ingo’s https://openlife.cc/system/files/4-modifications-for-Raft-consensus.pdf[_Four modifications for the Raft consensus algorithm_] paper. It enables the creation of a self-managing cluster of nodes by transforming an application into a Raft node. Raftify is meant to be a more _cost-efficient_ alternative to running a validator cluster with a separate full-fledged Raft cluster and a permission log.

This Go package is designed to be directly embedded into an application and provide a direct way of communicating between individual nodes, omitting the additional overhead caused by replicating a log.

== Concepts

Raftify takes the leader election algorithm of the Raft consensus and extends its functionality with a couple of new concenptual layers for increased robustness.

=== Leader Stickiness

The Raft consensus holds up pretty well in an optimal scenario. There is, however, a real-world scenario, albeit a corner case, in which it doesn’t hold up so well. This case is described as “flip-flopping” and happens whenever two nodes in the cluster constantly compete for leadership. This can be caused by flaky networks where connectivity between two adjacent nodes (one of which is the current leader) breaks down multiple times for a few seconds.

Raftify's first extension solves this problem by means of a prevoting phase which comes with an additional node state: the _PreCandidate_. The prevoting step falls in between the follower and the candidate state and is used to make sure that a new election _is really necessary_. So, once the election timeout on a follower elapses, it won't immediately switch to the candidate state but to the precandidate state. It then starts collecting prevotes in order to make sure the other nodes haven’t heard back from the leader either and the leader is in fact unavailable, not just from its own perspective but from the other nodes’ perspectives, too. If the precandidate reaches a quorum of prevotes, it will switch into the candidate state and initiate a new election cycle.

The way the prevoting phase differs from the voting phase is that the rules slightly differ. While candidates do not grant their personal vote to anyone else than themselves and only accept votes from followers, precandidates grant their personal prevote to any precandidate requesting it. If the majority of nodes have become precandidates the latest precandidate will be able to reach the quorum of `(n/2)+1` prevotes and switch into the candidate state.

=== Network Partition Tolerance

The second extension Raftify implements is a set of mechanisms that allows the cluster to continue operation during network partitions while keeping the single-leader promise.

==== Leader Self-Awareness

Network partitions cause connections between nodes to break down and split the cluster into multiple smaller sub-clusters. By default, Raft allows nodes in a sub-cluster to elect a new leader if the quorum of votes needed for promotion can be reached, thus making it possible for multiple leaders to coexist in different sub-clusters. In a validator cluster, this is a huge problem as the leader is meant to be the sole node with permission to sign messages, so multiple leaders are a sure-fire way to double-siging.

Raftify solves this problem by means of a _leader quorum_. Every single time a leader sends out heartbeat to all of the follower nodes, it keeps track of which node has replied in a short time frame (200ms with best performance option). If the leader can't get a quorum of heartbeat replies three times in a row, it will suspect it got split off into a sub-cluster which cannot reach the quorum anymore. This, in turn, means that the other sub-cluster may still be able to reach the quorum and elect a new leader, so it voluntarily steps down after 3x200ms which is before the minimum election timeout of 800ms on any other cluster node runs out, keeping the promise of one single leader at all times at the cost of no leader for a short time frame (at least 200ms with best performance option).

==== Two-Stage Quorum

Raftify uses hashicorp's memberlist library to manage cluster membership and failure detection which means that it detects unavailable nodes and kicks them after a short period of time, thus enabling dynamic resizing of the cluster for failure-related events. By extension, this dynamically changing cluster size also affects the cluster's quorum. This bears the risk that a long lasting network partition could permanently split a cluster into two independent clusters which would both be able to reach their own quorum and elect their own leader.

This is where the two-stage quorum comes into play. It simply keeps the quorum of the previous cluster size as a requirement for establishing the quorum for the new/reduced cluster size. This way, even split-off partitions with an adjusted cluster size won't be able to elect a new leader for the duration of the partition.

== Limitations

* A cluster size of n can tolerate up to `floor((n-1)/2)`` node failures.
** Example: A cluster of 5 nodes tolerates `floor((5-1)/2) = 2` node failures.
* There must never fail more than `floor((n-1)/2)`_` nodes at the same time. Once the failed nodes are kicked out of the memberlist and the size shrinks, the tolerance resets to the new reduced cluster size.
** Example 1: If in a cluster of 5 nodes 3 nodes fail in a short time frame, the remaining 2 nodes will never be able to reach the quorum again in order to negotiate a new leader.
** Example 2: If in a cluster of 5 nodes 2 nodes fail in a short time frame, the remaining 3 nodes will still be able to reach the quorum in order to negotiate a new leader. The crashed nodes will eventually be kicked from the memberlist, thus shrinking the cluster size to a total of 3 nodes and adjusting its failure tolerance to `floor((3-1)/2) = 1` node.

== Configuration Reference

The configuration is to be provided in a `raftify.json` file. It needs to be put into the working directory specified in the second parameter of the `InitNode` method. For Gaia, this would be `~/.gaiad/config/`.

[cols="1,1,5"]
|===
|Key|Value|Description

|id|string|*(Mandatory)* The node’s identifier.
Must be unique.

|max_nodes|int|*(Mandatory)* The self-imposed limit of nodes to be run in the cluster.
Must be greater than 0.

|expect|int|*(Mandatory)* The number of nodes expected to be online in order to bootstrap the cluster and start the leader election. Once the expected number of nodes is online, all cluster members will be started simultaneously.
Must be 1 or higher.
*WARNING:* Please use `expect = 1` for single-node setups only. If you plan on running more than one node, set the `expect` value to the final cluster size on **ALL** nodes. 

|encrypt|string|_(Optional)_ The hex representation of the secret key used to encrypt messages.
The value must be either 16, 24 or 32 bytes to select AES-128, AES-192 or AES-256. https://www.browserling.com/tools/random-bytes[Use this tool to generate a key.]

|performance|int|_(Optional)_ The modifier used to multiply the maximum and minimum timeout and ticker settings. Higher values increase leader stability and reduce bandwidth and CPU but also increase the time needed to recover from a leader failure.
Must be 1 or higher. Defaults to 1 which is also the maximum performance setting.

|log_level|string|_(Optional)_ The minimum log level for console log messages. Can be DEBUG, INFO, WARN, ERR. Defaults to WARN.

|bind_addr|string|_(Optional)_ The address to bind to.
Defaults to 0.0.0.0 (all interfaces).

|bind_port|string|_(Optional)_ The port to bind to.
Defaults to 7946 (default port of memberlist).

|peer_list|[]string|_(Optional)_ The list of IP addresses of all cluster members (optionally including the address of the local node). It is used to determine the quorum in a non-bootstrapped cluster.
For example, if your peerlist has `n = 3` nodes then `floor((n/2)+1) = 2` nodes will need to be up and running to bootstrap the cluster.
Addresses must be provided in the `host:port` format.
Must not be empty if more than one node is expected.

|===

=== API

[source,go]
----
func InitNode(logger *log.Logger, workingDir string) (*Node, error)
----

Initializes a new Raftify node. Blocks until the cluster is successfully bootstrapped.

[source,go]
----
func (n *Node) Shutdown() error
----

Gracefully shuts down the Raftify node. All timers/tickers and listeners are stopped, channels are closed and the node leaves the cluster.

[source,go]
----
func (n *Node) GetHealthScore() int
----

Returns the health score which is a metric from the hashicorp/memberlist library. Lower numbers
are better, and 0 means "totally healthy".

[source,go]
----
func (n *Node) GetMembers() map[string]string
----

Returns a map of all members listed in the local memberlist with their respective id and address.

[source,go]
----
func (n *Node) GetState() State
----

Returns the node's current state which is either Leader, Follower, PreCandidate or Candidate.

== Optional Features/Improvements

[cold="3*"]
|===
|Current state|Proposed changes|Desired effect

|Intended and unintended leave events are internally handled the same. There’s no difference between a node being shut down and a crashed node leaving the cluster.|Implement custom message to be broadcasted alongside the default events that triggers an immediate change of the cluster size for intended leave events and therefore also the quorum.|Makes sure that only failover scenarios are backed by the constraint of having to reach the quorum of the previous cluster size. A cluster with 2 nodes for example could be shrunk to a single-node cluster and keep running despite the majority of nodes taken offline.

|Once the expected number of nodes are online and the cluster is bootstrapped, the nodes go through the full election process in order to elect their first leader.|Make the first node to start up the first leader on successful bootstrap. This can be measured by how many peers could be reached. If a node reaches no peers, it means that it started up first and thus it will skip the precandidate and candidate states and immediately become the first leader.|This skips the delay associated with the prevoting and voting phase needed to elect the first leader in order to get things going. This saves a few seconds on startup at best, so it’s nice to have.
|===
